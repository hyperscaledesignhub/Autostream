[0m[1mdata.aws_region.current: Reading...[0m[0m
[0m[1mmodule.eks.module.kms.data.aws_caller_identity.current[0]: Reading...[0m[0m
[0m[1mmodule.eks.data.aws_caller_identity.current: Reading...[0m[0m
[0m[1mmodule.eks.data.aws_partition.current: Reading...[0m[0m
[0m[1mmodule.eks.module.kms.data.aws_partition.current[0]: Reading...[0m[0m
[0m[1mdata.aws_caller_identity.current: Reading...[0m[0m
[0m[1maws_ecr_repository.demo_app: Refreshing state... [id=fluss-demo][0m
[0m[1mmodule.eks.aws_cloudwatch_log_group.this[0]: Refreshing state... [id=/aws/eks/fluss-eks-cluster/cluster][0m
[0m[1maws_ecr_repository.fluss: Refreshing state... [id=fluss][0m
[0m[1mmodule.vpc.aws_vpc.this[0]: Refreshing state... [id=vpc-0bfdc16339792fb6c][0m
[0m[1mmodule.eks.module.kms.data.aws_partition.current[0]: Read complete after 0s [id=aws][0m
[0m[1mdata.aws_region.current: Read complete after 0s [id=us-west-2][0m
[0m[1mmodule.eks.data.aws_partition.current: Read complete after 0s [id=aws][0m
[0m[1mdata.aws_availability_zones.available: Reading...[0m[0m
[0m[1mmodule.eks.data.aws_iam_policy_document.assume_role_policy[0]: Reading...[0m[0m
[0m[1mmodule.eks.data.aws_iam_policy_document.assume_role_policy[0]: Read complete after 0s [id=2764486067][0m
[0m[1mmodule.eks.aws_iam_role.this[0]: Refreshing state... [id=fluss-eks-cluster-cluster-20251122164419826800000001][0m
[0m[1mmodule.eks.data.aws_caller_identity.current: Read complete after 1s [id=343218179954][0m
[0m[1mmodule.eks.data.aws_iam_session_context.current: Reading...[0m[0m
[0m[1mmodule.eks.data.aws_iam_session_context.current: Read complete after 0s [id=arn:aws:iam::343218179954:user/streampipeline][0m
[0m[1mdata.aws_caller_identity.current: Read complete after 1s [id=343218179954][0m
[0m[1mmodule.eks.module.kms.data.aws_caller_identity.current[0]: Read complete after 1s [id=343218179954][0m
[0m[1mdata.aws_availability_zones.available: Read complete after 2s [id=us-west-2][0m
[0m[1maws_ecr_lifecycle_policy.demo_app: Refreshing state... [id=fluss-demo][0m
[0m[1maws_ecr_lifecycle_policy.fluss: Refreshing state... [id=fluss][0m
[0m[1mmodule.eks.aws_iam_role_policy_attachment.this["AmazonEKSClusterPolicy"]: Refreshing state... [id=fluss-eks-cluster-cluster-20251122164419826800000001-20251122164422425800000002][0m
[0m[1mmodule.eks.aws_iam_role_policy_attachment.this["AmazonEKSVPCResourceController"]: Refreshing state... [id=fluss-eks-cluster-cluster-20251122164419826800000001-20251122164422696000000003][0m
[0m[1mmodule.eks.module.kms.data.aws_iam_policy_document.this[0]: Reading...[0m[0m
[0m[1mmodule.eks.module.kms.data.aws_iam_policy_document.this[0]: Read complete after 0s [id=2823212091][0m
[0m[1mmodule.eks.module.kms.aws_kms_key.this[0]: Refreshing state... [id=563f5b2f-4c4a-4bad-8c60-8526ec8aaeff][0m
[0m[1mmodule.vpc.aws_default_security_group.this[0]: Refreshing state... [id=sg-0beb03d47661cfbbd][0m
[0m[1mmodule.vpc.aws_default_route_table.default[0]: Refreshing state... [id=rtb-0363a863cd4abd064][0m
[0m[1mmodule.vpc.aws_default_network_acl.this[0]: Refreshing state... [id=acl-02290289f48722cc0][0m
[0m[1mmodule.vpc.aws_route_table.private[1]: Refreshing state... [id=rtb-080ce9e6a54f1c497][0m
[0m[1mmodule.vpc.aws_route_table.private[0]: Refreshing state... [id=rtb-09d57dea585a51f9a][0m
[0m[1mmodule.eks.aws_security_group.cluster[0]: Refreshing state... [id=sg-0df78a6c2542cf9cf][0m
[0m[1mmodule.vpc.aws_internet_gateway.this[0]: Refreshing state... [id=igw-052e953ac9a0c9e53][0m
[0m[1mmodule.vpc.aws_subnet.public[0]: Refreshing state... [id=subnet-0d610b397539bd03f][0m
[0m[1mmodule.eks.aws_security_group.node[0]: Refreshing state... [id=sg-0098304a6ff50c7ad][0m
[0m[1mmodule.vpc.aws_subnet.public[1]: Refreshing state... [id=subnet-00659ab18c52dc228][0m
[0m[1mmodule.vpc.aws_subnet.private[1]: Refreshing state... [id=subnet-0d0ab0150e308786e][0m
[0m[1mmodule.vpc.aws_subnet.private[0]: Refreshing state... [id=subnet-0f9c767879ba0825d][0m
[0m[1mmodule.vpc.aws_route_table.public[0]: Refreshing state... [id=rtb-05593d46bcf00d8df][0m
[0m[1mmodule.vpc.aws_eip.nat[0]: Refreshing state... [id=eipalloc-0ecd9fe8d2b4ae129][0m
[0m[1mmodule.vpc.aws_eip.nat[1]: Refreshing state... [id=eipalloc-02165d1f94d073ea5][0m
[0m[1mmodule.eks.aws_security_group_rule.node["egress_all"]: Refreshing state... [id=sgrule-1673041911][0m
[0m[1mmodule.eks.aws_security_group_rule.node["ingress_cluster_4443_webhook"]: Refreshing state... [id=sgrule-2158458585][0m
[0m[1mmodule.eks.aws_security_group_rule.node["ingress_cluster_6443_webhook"]: Refreshing state... [id=sgrule-1239748943][0m
[0m[1mmodule.eks.aws_security_group_rule.node["ingress_self_coredns_udp"]: Refreshing state... [id=sgrule-2085473125][0m
[0m[1mmodule.eks.aws_security_group_rule.node["ingress_cluster_8443_webhook"]: Refreshing state... [id=sgrule-1522085230][0m
[0m[1mmodule.eks.aws_security_group_rule.node["ingress_cluster_9443_webhook"]: Refreshing state... [id=sgrule-1041768101][0m
[0m[1mmodule.eks.aws_security_group_rule.node["ingress_nodes_ephemeral"]: Refreshing state... [id=sgrule-1236733021][0m
[0m[1mmodule.eks.aws_security_group_rule.node["ingress_cluster_kubelet"]: Refreshing state... [id=sgrule-2673839366][0m
[0m[1mmodule.eks.aws_security_group_rule.node["ingress_cluster_443"]: Refreshing state... [id=sgrule-3077128826][0m
[0m[1mmodule.eks.aws_security_group_rule.node["ingress_self_coredns_tcp"]: Refreshing state... [id=sgrule-3625085066][0m
[0m[1mmodule.eks.aws_security_group_rule.cluster["ingress_nodes_443"]: Refreshing state... [id=sgrule-1170360687][0m
[0m[1mmodule.vpc.aws_route_table_association.private[1]: Refreshing state... [id=rtbassoc-05bf4e81c22203f98][0m
[0m[1mmodule.vpc.aws_route_table_association.private[0]: Refreshing state... [id=rtbassoc-062ca9971cb70aa41][0m
[0m[1mmodule.vpc.aws_route_table_association.public[1]: Refreshing state... [id=rtbassoc-0c4f86a7402747c1f][0m
[0m[1mmodule.vpc.aws_route_table_association.public[0]: Refreshing state... [id=rtbassoc-0859d6032fe91d082][0m
[0m[1mmodule.vpc.aws_route.public_internet_gateway[0]: Refreshing state... [id=r-rtb-05593d46bcf00d8df1080289494][0m
[0m[1mmodule.eks.module.kms.aws_kms_alias.this["cluster"]: Refreshing state... [id=alias/eks/fluss-eks-cluster][0m
[0m[1mmodule.eks.aws_iam_policy.cluster_encryption[0]: Refreshing state... [id=arn:aws:iam::343218179954:policy/fluss-eks-cluster-cluster-ClusterEncryption20251122164448773400000015][0m
[0m[1mmodule.vpc.aws_nat_gateway.this[1]: Refreshing state... [id=nat-0ae746eac6fc607c4][0m
[0m[1mmodule.vpc.aws_nat_gateway.this[0]: Refreshing state... [id=nat-0a91590a49f4900b6][0m
[0m[1mmodule.eks.module.eks_managed_node_group["tablet_server"].data.aws_caller_identity.current: Reading...[0m[0m
[0m[1mmodule.eks.module.eks_managed_node_group["tablet_server"].data.aws_partition.current: Reading...[0m[0m
[0m[1mmodule.eks.module.eks_managed_node_group["coordinator"].data.aws_caller_identity.current: Reading...[0m[0m
[0m[1mmodule.eks.module.eks_managed_node_group["tablet_server"].data.aws_partition.current: Read complete after 0s [id=aws][0m
[0m[1mmodule.eks.module.eks_managed_node_group["coordinator"].data.aws_partition.current: Reading...[0m[0m
[0m[1mmodule.eks.module.eks_managed_node_group["coordinator"].data.aws_partition.current: Read complete after 0s [id=aws][0m
[0m[1mmodule.eks.module.eks_managed_node_group["coordinator"].data.aws_iam_policy_document.assume_role_policy[0]: Reading...[0m[0m
[0m[1mmodule.eks.module.eks_managed_node_group["tablet_server"].data.aws_iam_policy_document.assume_role_policy[0]: Reading...[0m[0m
[0m[1mmodule.eks.module.eks_managed_node_group["coordinator"].data.aws_iam_policy_document.assume_role_policy[0]: Read complete after 0s [id=2560088296][0m
[0m[1mmodule.eks.module.eks_managed_node_group["tablet_server"].data.aws_iam_policy_document.assume_role_policy[0]: Read complete after 0s [id=2560088296][0m
[0m[1mmodule.eks.module.eks_managed_node_group["coordinator"].aws_iam_role.this[0]: Refreshing state... [id=coordinator-eks-node-group-2025112216443807770000000a][0m
[0m[1mmodule.eks.module.eks_managed_node_group["tablet_server"].aws_iam_role.this[0]: Refreshing state... [id=tablet-server-eks-node-group-20251122164438077600000009][0m
[0m[1mmodule.eks.aws_eks_cluster.this[0]: Refreshing state... [id=fluss-eks-cluster][0m
[0m[1mmodule.vpc.aws_route.private_nat_gateway[0]: Refreshing state... [id=r-rtb-09d57dea585a51f9a1080289494][0m
[0m[1mmodule.vpc.aws_route.private_nat_gateway[1]: Refreshing state... [id=r-rtb-080ce9e6a54f1c4971080289494][0m
[0m[1mmodule.eks.module.eks_managed_node_group["coordinator"].data.aws_caller_identity.current: Read complete after 0s [id=343218179954][0m
[0m[1mmodule.eks.module.eks_managed_node_group["tablet_server"].data.aws_caller_identity.current: Read complete after 0s [id=343218179954][0m
[0m[1mmodule.eks.aws_iam_role_policy_attachment.cluster_encryption[0]: Refreshing state... [id=fluss-eks-cluster-cluster-20251122164419826800000001-20251122164449970000000016][0m
[0m[1mmodule.eks.module.eks_managed_node_group["tablet_server"].aws_iam_role_policy_attachment.additional["AmazonEBSCSIDriverPolicy"]: Refreshing state... [id=tablet-server-eks-node-group-20251122164438077600000009-2025112216443947920000000d][0m
[0m[1mmodule.eks.module.eks_managed_node_group["tablet_server"].aws_iam_role_policy_attachment.this["arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryReadOnly"]: Refreshing state... [id=tablet-server-eks-node-group-20251122164438077600000009-2025112216443943380000000b][0m
[0m[1mmodule.eks.module.eks_managed_node_group["tablet_server"].aws_iam_role_policy_attachment.this["arn:aws:iam::aws:policy/AmazonEKSWorkerNodePolicy"]: Refreshing state... [id=tablet-server-eks-node-group-20251122164438077600000009-20251122164440097900000012][0m
[0m[1mmodule.eks.module.eks_managed_node_group["coordinator"].aws_iam_role_policy_attachment.this["arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryReadOnly"]: Refreshing state... [id=coordinator-eks-node-group-2025112216443807770000000a-2025112216443951900000000e][0m
[0m[1mmodule.eks.module.eks_managed_node_group["coordinator"].aws_iam_role_policy_attachment.this["arn:aws:iam::aws:policy/AmazonEKS_CNI_Policy"]: Refreshing state... [id=coordinator-eks-node-group-2025112216443807770000000a-20251122164440097900000011][0m
[0m[1mmodule.eks.module.eks_managed_node_group["coordinator"].aws_iam_role_policy_attachment.additional["AmazonEBSCSIDriverPolicy"]: Refreshing state... [id=coordinator-eks-node-group-2025112216443807770000000a-2025112216443946890000000c][0m
[0m[1mmodule.eks.module.eks_managed_node_group["tablet_server"].aws_iam_role_policy_attachment.this["arn:aws:iam::aws:policy/AmazonEKS_CNI_Policy"]: Refreshing state... [id=tablet-server-eks-node-group-20251122164438077600000009-2025112216443998840000000f][0m
[0m[1mmodule.eks.module.eks_managed_node_group["coordinator"].aws_iam_role_policy_attachment.this["arn:aws:iam::aws:policy/AmazonEKSWorkerNodePolicy"]: Refreshing state... [id=coordinator-eks-node-group-2025112216443807770000000a-20251122164440039500000010][0m
[0m[1mmodule.eks.data.aws_eks_addon_version.this["vpc-cni"]: Reading...[0m[0m
[0m[1mmodule.eks.time_sleep.this[0]: Refreshing state... [id=2025-11-22T16:54:40Z][0m
[0m[1mmodule.eks.data.aws_eks_addon_version.this["coredns"]: Reading...[0m[0m
[0m[1mmodule.eks.data.aws_eks_addon_version.this["kube-proxy"]: Reading...[0m[0m
[0m[1mmodule.eks.aws_ec2_tag.cluster_primary_security_group["Environment"]: Refreshing state... [id=sg-092250e94563a2ede,Environment][0m
[0m[1mmodule.eks.data.tls_certificate.this[0]: Reading...[0m[0m
[0m[1mmodule.eks.aws_ec2_tag.cluster_primary_security_group["ManagedBy"]: Refreshing state... [id=sg-092250e94563a2ede,ManagedBy][0m
[0m[1mmodule.eks.aws_ec2_tag.cluster_primary_security_group["Project"]: Refreshing state... [id=sg-092250e94563a2ede,Project][0m
[0m[1mkubernetes_namespace.fluss: Refreshing state... [id=fluss][0m
[0m[1mmodule.eks.data.aws_eks_addon_version.this["coredns"]: Read complete after 0s [id=coredns][0m
[0m[1mmodule.eks.data.aws_eks_addon_version.this["vpc-cni"]: Read complete after 0s [id=vpc-cni][0m
[0m[1mmodule.eks.module.eks_managed_node_group["tablet_server"].aws_launch_template.this[0]: Refreshing state... [id=lt-07d9106a955f56729][0m
[0m[1mmodule.eks.module.eks_managed_node_group["coordinator"].aws_launch_template.this[0]: Refreshing state... [id=lt-0601e4437be944635][0m
[0m[1mmodule.eks.data.aws_eks_addon_version.this["kube-proxy"]: Read complete after 1s [id=kube-proxy][0m
[0m[1mmodule.eks.module.eks_managed_node_group["tablet_server"].aws_eks_node_group.this[0]: Refreshing state... [id=fluss-eks-cluster:tablet-server-2025112216544818320000001b][0m
[0m[1mmodule.eks.module.eks_managed_node_group["coordinator"].aws_eks_node_group.this[0]: Refreshing state... [id=fluss-eks-cluster:coordinator-2025112216544818400000001d][0m
[0m[1mmodule.eks.data.tls_certificate.this[0]: Read complete after 1s [id=4687b9863b4f2caa907e2ad6c9ffcf73433dc367][0m
[0m[1mmodule.eks.aws_iam_openid_connect_provider.oidc_provider[0]: Refreshing state... [id=arn:aws:iam::343218179954:oidc-provider/oidc.eks.us-west-2.amazonaws.com/id/6532A7F86D4898E1EE12F2E3E7877B4E][0m
[0m[1mmodule.eks.aws_eks_addon.this["coredns"]: Refreshing state... [id=fluss-eks-cluster:coredns][0m
[0m[1mmodule.eks.aws_eks_addon.this["vpc-cni"]: Refreshing state... [id=fluss-eks-cluster:vpc-cni][0m
[0m[1mmodule.eks.aws_eks_addon.this["kube-proxy"]: Refreshing state... [id=fluss-eks-cluster:kube-proxy][0m
[0m[1mmodule.eks.kubernetes_config_map_v1_data.aws_auth[0]: Refreshing state... [id=kube-system/aws-auth][0m
[0m[1mkubernetes_service.zookeeper: Refreshing state... [id=fluss/zk-svc][0m
[0m[1mmodule.ebs_csi_irsa[0].data.aws_caller_identity.current: Reading...[0m[0m
[0m[1mmodule.ebs_csi_irsa[0].data.aws_partition.current: Reading...[0m[0m
[0m[1mmodule.ebs_csi_irsa[0].data.aws_region.current: Reading...[0m[0m
[0m[1mmodule.ebs_csi_irsa[0].data.aws_partition.current: Read complete after 0s [id=aws][0m
[0m[1mmodule.ebs_csi_irsa[0].data.aws_region.current: Read complete after 0s [id=us-west-2][0m
[0m[1mmodule.ebs_csi_irsa[0].data.aws_iam_policy_document.ebs_csi[0]: Reading...[0m[0m
[0m[1mmodule.ebs_csi_irsa[0].data.aws_iam_policy_document.ebs_csi[0]: Read complete after 0s [id=4189668531][0m
[0m[1mmodule.ebs_csi_irsa[0].aws_iam_policy.ebs_csi[0]: Refreshing state... [id=arn:aws:iam::343218179954:policy/AmazonEKS_EBS_CSI_Policy-20251122165737539800000022][0m
[0m[1mmodule.ebs_csi_irsa[0].data.aws_caller_identity.current: Read complete after 0s [id=343218179954][0m
[0m[1mmodule.ebs_csi_irsa[0].data.aws_iam_policy_document.this[0]: Reading...[0m[0m
[0m[1mmodule.ebs_csi_irsa[0].data.aws_iam_policy_document.this[0]: Read complete after 0s [id=3545344473][0m
[0m[1mmodule.ebs_csi_irsa[0].aws_iam_role.this[0]: Refreshing state... [id=fluss-eks-cluster-ebs-csi-driver][0m
[0m[1mkubernetes_stateful_set.zookeeper: Refreshing state... [id=fluss/zk][0m
[0m[1mmodule.ebs_csi_irsa[0].aws_iam_role_policy_attachment.ebs_csi[0]: Refreshing state... [id=fluss-eks-cluster-ebs-csi-driver-20251122165740471200000023][0m
[0m[1mhelm_release.fluss: Refreshing state... [id=fluss][0m
[0m[1maws_eks_addon.ebs_csi_driver[0]: Refreshing state... [id=fluss-eks-cluster:aws-ebs-csi-driver][0m
[0m[1mkubernetes_job.flink_aggregator: Refreshing state... [id=fluss/flink-aggregator][0m

Terraform used the selected providers to generate the following execution
plan. Resource actions are indicated with the following symbols:
  [32m+[0m create[0m

Terraform will perform the following actions:

[1m  # kubernetes_job.flink_aggregator[0m will be created
[0m  [32m+[0m[0m resource "kubernetes_job" "flink_aggregator" {
      [32m+[0m[0m id                  = (known after apply)
      [32m+[0m[0m wait_for_completion = false

      [32m+[0m[0m metadata {
          [32m+[0m[0m generation       = (known after apply)
          [32m+[0m[0m labels           = {
              [32m+[0m[0m "app" = "flink-aggregator"
            }
          [32m+[0m[0m name             = "flink-aggregator"
          [32m+[0m[0m namespace        = "fluss"
          [32m+[0m[0m resource_version = (known after apply)
          [32m+[0m[0m uid              = (known after apply)
        }

      [32m+[0m[0m spec {
          [32m+[0m[0m backoff_limit              = 0
          [32m+[0m[0m completion_mode            = (known after apply)
          [32m+[0m[0m completions                = 1
          [32m+[0m[0m parallelism                = 1
          [32m+[0m[0m ttl_seconds_after_finished = "86400"

          [32m+[0m[0m selector (known after apply)

          [32m+[0m[0m template {
              [32m+[0m[0m metadata {
                  [32m+[0m[0m generation       = (known after apply)
                  [32m+[0m[0m labels           = {
                      [32m+[0m[0m "app" = "flink-aggregator"
                    }
                  [32m+[0m[0m name             = (known after apply)
                  [32m+[0m[0m resource_version = (known after apply)
                  [32m+[0m[0m uid              = (known after apply)
                }
              [32m+[0m[0m spec {
                  [32m+[0m[0m automount_service_account_token  = true
                  [32m+[0m[0m dns_policy                       = "ClusterFirst"
                  [32m+[0m[0m enable_service_links             = true
                  [32m+[0m[0m host_ipc                         = false
                  [32m+[0m[0m host_network                     = false
                  [32m+[0m[0m host_pid                         = false
                  [32m+[0m[0m hostname                         = (known after apply)
                  [32m+[0m[0m node_name                        = (known after apply)
                  [32m+[0m[0m restart_policy                   = "Never"
                  [32m+[0m[0m scheduler_name                   = (known after apply)
                  [32m+[0m[0m service_account_name             = (known after apply)
                  [32m+[0m[0m share_process_namespace          = false
                  [32m+[0m[0m termination_grace_period_seconds = 30

                  [32m+[0m[0m container {
                      [32m+[0m[0m args                       = [
                          [32m+[0m[0m "-cp",
                          [32m+[0m[0m "/app/fluss-flink-realtime-demo.jar",
                          [32m+[0m[0m "com.example.fluss.flink.FlinkSensorAggregatorJob",
                          [32m+[0m[0m "--bootstrap",
                          [32m+[0m[0m "coordinator-server-hs.fluss.svc.cluster.local:9124",
                          [32m+[0m[0m "--database",
                          [32m+[0m[0m "iot",
                          [32m+[0m[0m "--table",
                          [32m+[0m[0m "sensor_readings",
                          [32m+[0m[0m "--window-minutes",
                          [32m+[0m[0m "1",
                        ]
                      [32m+[0m[0m command                    = [
                          [32m+[0m[0m "/app/entrypoint.sh",
                        ]
                      [32m+[0m[0m image                      = "343218179954.dkr.ecr.us-west-2.amazonaws.com/fluss-demo:latest"
                      [32m+[0m[0m image_pull_policy          = (known after apply)
                      [32m+[0m[0m name                       = "aggregator"
                      [32m+[0m[0m stdin                      = false
                      [32m+[0m[0m stdin_once                 = false
                      [32m+[0m[0m termination_message_path   = "/dev/termination-log"
                      [32m+[0m[0m termination_message_policy = (known after apply)
                      [32m+[0m[0m tty                        = false

                      [32m+[0m[0m resources {
                          [32m+[0m[0m limits   = {
                              [32m+[0m[0m "cpu"    = "1000m"
                              [32m+[0m[0m "memory" = "2Gi"
                            }
                          [32m+[0m[0m requests = {
                              [32m+[0m[0m "cpu"    = "500m"
                              [32m+[0m[0m "memory" = "1Gi"
                            }
                        }

                      [32m+[0m[0m security_context {
                          [32m+[0m[0m allow_privilege_escalation = true
                          [32m+[0m[0m privileged                 = false
                          [32m+[0m[0m read_only_root_filesystem  = false
                          [32m+[0m[0m run_as_user                = "0"
                        }
                    }

                  [32m+[0m[0m image_pull_secrets (known after apply)

                  [32m+[0m[0m init_container {
                      [32m+[0m[0m command                    = [
                          [32m+[0m[0m "sh",
                          [32m+[0m[0m "-c",
                          [32m+[0m[0m <<-EOT
                                echo "Waiting for Fluss coordinator to be ready..."
                                COORD_HOST="coordinator-server-hs.fluss.svc.cluster.local"
                                MAX_ATTEMPTS=60
                                ATTEMPT=0
                                while [ $ATTEMPT -lt $MAX_ATTEMPTS ]; do
                                  if nc -zv -w 2 "$COORD_HOST" 9124 2>&1 | grep -q "open\|succeeded"; then
                                    echo "Fluss coordinator is ready!"
                                    exit 0
                                  fi
                                  ATTEMPT=$((ATTEMPT + 1))
                                  echo "Waiting for Fluss coordinator... (attempt $ATTEMPT/$MAX_ATTEMPTS)"
                                  sleep 2
                                done
                                echo "ERROR: Fluss coordinator did not become ready after $MAX_ATTEMPTS attempts"
                                exit 1
                            EOT,
                        ]
                      [32m+[0m[0m image                      = "busybox:1.36"
                      [32m+[0m[0m image_pull_policy          = (known after apply)
                      [32m+[0m[0m name                       = "wait-for-fluss"
                      [32m+[0m[0m stdin                      = false
                      [32m+[0m[0m stdin_once                 = false
                      [32m+[0m[0m termination_message_path   = "/dev/termination-log"
                      [32m+[0m[0m termination_message_policy = (known after apply)
                      [32m+[0m[0m tty                        = false

                      [32m+[0m[0m resources (known after apply)
                    }
                  [32m+[0m[0m init_container {
                      [32m+[0m[0m command                    = [
                          [32m+[0m[0m "sh",
                          [32m+[0m[0m "-c",
                          [32m+[0m[0m <<-EOT
                                echo "Waiting for producer to start and create database 'iot'..."
                                echo "This init container waits 30 seconds to give the producer time to create the database"
                                sleep 30
                                echo "Proceeding - producer should have created the database by now"
                                exit 0
                            EOT,
                        ]
                      [32m+[0m[0m image                      = "busybox:1.36"
                      [32m+[0m[0m image_pull_policy          = (known after apply)
                      [32m+[0m[0m name                       = "wait-for-producer-database"
                      [32m+[0m[0m stdin                      = false
                      [32m+[0m[0m stdin_once                 = false
                      [32m+[0m[0m termination_message_path   = "/dev/termination-log"
                      [32m+[0m[0m termination_message_policy = (known after apply)
                      [32m+[0m[0m tty                        = false

                      [32m+[0m[0m resources (known after apply)
                    }

                  [32m+[0m[0m readiness_gate (known after apply)
                }
            }
        }

      [32m+[0m[0m timeouts {
          [32m+[0m[0m create = "2m"
          [32m+[0m[0m delete = "10m"
          [32m+[0m[0m update = "10m"
        }
    }

[1m  # kubernetes_job.producer[0m will be created
[0m  [32m+[0m[0m resource "kubernetes_job" "producer" {
      [32m+[0m[0m id                  = (known after apply)
      [32m+[0m[0m wait_for_completion = false

      [32m+[0m[0m metadata {
          [32m+[0m[0m generation       = (known after apply)
          [32m+[0m[0m labels           = {
              [32m+[0m[0m "app" = "fluss-producer"
            }
          [32m+[0m[0m name             = "fluss-producer"
          [32m+[0m[0m namespace        = "fluss"
          [32m+[0m[0m resource_version = (known after apply)
          [32m+[0m[0m uid              = (known after apply)
        }

      [32m+[0m[0m spec {
          [32m+[0m[0m backoff_limit              = 0
          [32m+[0m[0m completion_mode            = (known after apply)
          [32m+[0m[0m completions                = 1
          [32m+[0m[0m parallelism                = 1
          [32m+[0m[0m ttl_seconds_after_finished = "86400"

          [32m+[0m[0m selector (known after apply)

          [32m+[0m[0m template {
              [32m+[0m[0m metadata {
                  [32m+[0m[0m generation       = (known after apply)
                  [32m+[0m[0m labels           = {
                      [32m+[0m[0m "app" = "fluss-producer"
                    }
                  [32m+[0m[0m name             = (known after apply)
                  [32m+[0m[0m resource_version = (known after apply)
                  [32m+[0m[0m uid              = (known after apply)
                }
              [32m+[0m[0m spec {
                  [32m+[0m[0m automount_service_account_token  = true
                  [32m+[0m[0m dns_policy                       = "ClusterFirst"
                  [32m+[0m[0m enable_service_links             = true
                  [32m+[0m[0m host_ipc                         = false
                  [32m+[0m[0m host_network                     = false
                  [32m+[0m[0m host_pid                         = false
                  [32m+[0m[0m hostname                         = (known after apply)
                  [32m+[0m[0m node_name                        = (known after apply)
                  [32m+[0m[0m restart_policy                   = "Never"
                  [32m+[0m[0m scheduler_name                   = (known after apply)
                  [32m+[0m[0m service_account_name             = (known after apply)
                  [32m+[0m[0m share_process_namespace          = false
                  [32m+[0m[0m termination_grace_period_seconds = 30

                  [32m+[0m[0m container {
                      [32m+[0m[0m args                       = [
                          [32m+[0m[0m "-jar",
                          [32m+[0m[0m "/app/fluss-flink-realtime-demo.jar",
                          [32m+[0m[0m "--bootstrap",
                          [32m+[0m[0m "coordinator-server-hs.fluss.svc.cluster.local:9124",
                          [32m+[0m[0m "--database",
                          [32m+[0m[0m "iot",
                          [32m+[0m[0m "--table",
                          [32m+[0m[0m "sensor_readings",
                          [32m+[0m[0m "--buckets",
                          [32m+[0m[0m "12",
                          [32m+[0m[0m "--rate",
                          [32m+[0m[0m "2000",
                          [32m+[0m[0m "--flush",
                          [32m+[0m[0m "5000",
                          [32m+[0m[0m "--stats",
                          [32m+[0m[0m "1000",
                        ]
                      [32m+[0m[0m command                    = [
                          [32m+[0m[0m "/app/entrypoint.sh",
                        ]
                      [32m+[0m[0m image                      = "343218179954.dkr.ecr.us-west-2.amazonaws.com/fluss-demo:latest"
                      [32m+[0m[0m image_pull_policy          = (known after apply)
                      [32m+[0m[0m name                       = "producer"
                      [32m+[0m[0m stdin                      = false
                      [32m+[0m[0m stdin_once                 = false
                      [32m+[0m[0m termination_message_path   = "/dev/termination-log"
                      [32m+[0m[0m termination_message_policy = (known after apply)
                      [32m+[0m[0m tty                        = false

                      [32m+[0m[0m resources {
                          [32m+[0m[0m limits   = {
                              [32m+[0m[0m "cpu"    = "500m"
                              [32m+[0m[0m "memory" = "1Gi"
                            }
                          [32m+[0m[0m requests = {
                              [32m+[0m[0m "cpu"    = "200m"
                              [32m+[0m[0m "memory" = "512Mi"
                            }
                        }

                      [32m+[0m[0m security_context {
                          [32m+[0m[0m allow_privilege_escalation = true
                          [32m+[0m[0m privileged                 = false
                          [32m+[0m[0m read_only_root_filesystem  = false
                          [32m+[0m[0m run_as_user                = "0"
                        }
                    }

                  [32m+[0m[0m image_pull_secrets (known after apply)

                  [32m+[0m[0m init_container {
                      [32m+[0m[0m command                    = [
                          [32m+[0m[0m "sh",
                          [32m+[0m[0m "-c",
                          [32m+[0m[0m <<-EOT
                                echo "Waiting for Fluss coordinator to be ready..."
                                COORD_HOST="coordinator-server-hs.fluss.svc.cluster.local"
                                MAX_ATTEMPTS=60
                                ATTEMPT=0
                                while [ $ATTEMPT -lt $MAX_ATTEMPTS ]; do
                                  if nc -zv -w 2 "$COORD_HOST" 9124 2>&1 | grep -q "open\|succeeded"; then
                                    echo "Fluss coordinator is ready!"
                                    exit 0
                                  fi
                                  ATTEMPT=$((ATTEMPT + 1))
                                  echo "Waiting for Fluss coordinator... (attempt $ATTEMPT/$MAX_ATTEMPTS)"
                                  sleep 2
                                done
                                echo "ERROR: Fluss coordinator did not become ready after $MAX_ATTEMPTS attempts"
                                exit 1
                            EOT,
                        ]
                      [32m+[0m[0m image                      = "busybox:1.36"
                      [32m+[0m[0m image_pull_policy          = (known after apply)
                      [32m+[0m[0m name                       = "wait-for-fluss"
                      [32m+[0m[0m stdin                      = false
                      [32m+[0m[0m stdin_once                 = false
                      [32m+[0m[0m termination_message_path   = "/dev/termination-log"
                      [32m+[0m[0m termination_message_policy = (known after apply)
                      [32m+[0m[0m tty                        = false

                      [32m+[0m[0m resources (known after apply)
                    }

                  [32m+[0m[0m readiness_gate (known after apply)
                }
            }
        }

      [32m+[0m[0m timeouts {
          [32m+[0m[0m create = "2m"
          [32m+[0m[0m delete = "10m"
          [32m+[0m[0m update = "10m"
        }
    }

[1mPlan:[0m 2 to add, 0 to change, 0 to destroy.
[0m[0m[1mkubernetes_job.producer: Creating...[0m[0m
[33mâ•·[0m[0m
[33mâ”‚[0m [0m[1m[33mWarning: [0m[0m[1mArgument is deprecated[0m
[33mâ”‚[0m [0m
[33mâ”‚[0m [0m[0m  with module.eks.aws_iam_role.this[0],
[33mâ”‚[0m [0m  on .terraform/modules/eks/main.tf line 293, in resource "aws_iam_role" "this":
[33mâ”‚[0m [0m 293: resource "aws_iam_role" "this" [4m{[0m[0m
[33mâ”‚[0m [0m
[33mâ”‚[0m [0minline_policy is deprecated. Use the aws_iam_role_policy resource instead.
[33mâ”‚[0m [0mIf Terraform should exclusively manage all inline policy associations (the
[33mâ”‚[0m [0mcurrent behavior of this argument), use the aws_iam_role_policies_exclusive
[33mâ”‚[0m [0mresource as well.
[33mâ”‚[0m [0m
[33mâ”‚[0m [0m(and 3 more similar warnings elsewhere)
[33mâ•µ[0m[0m
[31mâ•·[0m[0m
[31mâ”‚[0m [0m[1m[31mError: [0m[0m[1mFailed to create Job! API error: jobs.batch "fluss-producer" already exists[0m
[31mâ”‚[0m [0m
[31mâ”‚[0m [0m[0m  with kubernetes_job.producer,
[31mâ”‚[0m [0m  on jobs.tf line 2, in resource "kubernetes_job" "producer":
[31mâ”‚[0m [0m   2: resource "kubernetes_job" "producer" [4m{[0m[0m
[31mâ”‚[0m [0m
[31mâ•µ[0m[0m
