=== Deploying Kubernetes Resources ===
Namespace: fluss
Demo Image: 343218179954.dkr.ecr.us-west-2.amazonaws.com/fluss-demo:latest
Fluss Image: 343218179954.dkr.ecr.us-west-2.amazonaws.com/fluss

[1/8] Creating namespace...
namespace/fluss created
[2/8] Deploying ZooKeeper...
service/zk-svc created
statefulset.apps/zk created
Waiting for ZooKeeper to be ready...
pod/zk-0 condition met
[3/8] Deploying Fluss via Helm...
Release "fluss" does not exist. Installing it now.
NAME: fluss
LAST DEPLOYED: Fri Dec  5 14:59:50 2025
NAMESPACE: fluss
STATUS: deployed
REVISION: 1
TEST SUITE: None
[4/8] Deploying Flink cluster...
configmap/flink-config created
deployment.apps/flink-jobmanager created
service/flink-jobmanager created
statefulset.apps/flink-taskmanager created
service/flink-taskmanager created
[5/8] Deploying monitoring stack...
namespace/monitoring created
"prometheus-community" already exists with the same configuration, skipping
Hang tight while we grab the latest from your chart repositories...
...Successfully got an update from the "jetstack" chart repository
...Successfully got an update from the "apache" chart repository
...Successfully got an update from the "aws-ebs-csi-driver" chart repository
...Successfully got an update from the "clickhouse-operator" chart repository
...Successfully got an update from the "fluss" chart repository
...Successfully got an update from the "flink-operator-repo" chart repository
...Successfully got an update from the "streamnative" chart repository
...Successfully got an update from the "prometheus-community" chart repository
...Successfully got an update from the "bitnami" chart repository
Update Complete. ⎈Happy Helming!⎈
Release "prometheus" does not exist. Installing it now.
NAME: prometheus
LAST DEPLOYED: Fri Dec  5 15:00:40 2025
NAMESPACE: monitoring
STATUS: deployed
REVISION: 1
NOTES:
kube-prometheus-stack has been installed. Check its status by running:
  kubectl --namespace monitoring get pods -l "release=prometheus"

Visit https://github.com/prometheus-operator/kube-prometheus for instructions on how to create & configure Alertmanager and Prometheus instances using the Operator.
[6/9] Deploying producer job...
[6.1/9] Creating Fluss table with 48 buckets...
=== Creating Fluss Table with 48 Buckets ===
  Namespace: fluss
  Bootstrap: coordinator-server-hs.fluss.svc.cluster.local:9124
  Database: iot
  Table: sensor_readings
  Buckets: 48
  Image: 343218179954.dkr.ecr.us-west-2.amazonaws.com/fluss-demo:latest

Cleaning up any existing create-table job...
Creating table creation job...
job.batch/fluss-create-table created

Waiting for job to complete...
.......
✓ Job completed successfully!

Job logs:
Defaulted container "create-table" out of: create-table, wait-for-coordinator (init)
[main] INFO org.apache.fluss.metrics.registry.MetricRegistry - No metrics reporter configured, no metrics will be exposed/reported.
[main] INFO com.example.fluss.setup.CreateTableWithBuckets - Creating database 'iot' if it doesn't exist...
[main] INFO com.example.fluss.setup.CreateTableWithBuckets - Database 'iot' ready
[main] INFO com.example.fluss.setup.CreateTableWithBuckets - Table 'iot.sensor_readings' does not exist, will create it
[main] INFO com.example.fluss.setup.CreateTableWithBuckets - Creating table 'iot.sensor_readings' with 48 buckets...
[main] INFO com.example.fluss.setup.CreateTableWithBuckets - Successfully created table 'iot.sensor_readings' with 48 buckets
[main] INFO org.apache.fluss.rpc.netty.client.NettyClient - Netty client was shutdown successfully.
[main] INFO com.example.fluss.setup.CreateTableWithBuckets - Done!

=== Table Creation Complete ===
Table 'iot.sensor_readings' is ready with 48 buckets
You can now deploy the producer to use this table
  ✓ Table creation completed
[6.2/9] Deploying producer job...
job.batch/fluss-producer created
service/fluss-producer-metrics created
  ✓ Producer job deployed with defaults
[7/9] Deploying ServiceMonitors and PodMonitors for Prometheus...
servicemonitor.monitoring.coreos.com/fluss-producer-metrics created
servicemonitor.monitoring.coreos.com/flink-jobmanager-metrics created
servicemonitor.monitoring.coreos.com/flink-taskmanager-metrics created
servicemonitor.monitoring.coreos.com/fluss-coordinator-metrics created
servicemonitor.monitoring.coreos.com/fluss-tablet-metrics created
  ✓ ServiceMonitors deployed
podmonitor.monitoring.coreos.com/fluss-producer-pods created
podmonitor.monitoring.coreos.com/flink-jobmanager-pods created
podmonitor.monitoring.coreos.com/flink-taskmanager-pods created
  ✓ PodMonitors deployed
[8/9] Deploying Grafana dashboard...
configmap/fluss-flink-dashboard created
  ✓ Grafana dashboard ConfigMap deployed
  Importing dashboard via Grafana API...
  ✓ Dashboard imported successfully via Grafana API!
[9/9] Waiting for components to be ready...
  Waiting for Flink JobManager...
pod/flink-jobmanager-d8cd58899-vmbfn condition met
  Waiting for Flink TaskManagers...
pod/flink-taskmanager-0 condition met
pod/flink-taskmanager-1 condition met

=== Deployment Complete ===

Check status:
  kubectl get pods -n fluss
  kubectl get pods -n monitoring

Check Flink cluster:
  kubectl get pods -n fluss -l app=flink
  kubectl get nodes -l flink-component

Check monitoring:
  kubectl get servicemonitor -n fluss
  kubectl get podmonitor -n fluss

Access Flink Web UI:
  kubectl port-forward -n fluss svc/flink-jobmanager 8081:8081
  Then open: http://localhost:8081

Access Grafana:
  GRAFANA_SVC=$(kubectl get svc -n monitoring -l app.kubernetes.io/name=grafana -o jsonpath='{.items[0].metadata.name}')
  kubectl port-forward -n monitoring svc/$GRAFANA_SVC 3000:80
  Then open: http://localhost:3000
  Username: admin
  Password: admin123

Access Prometheus:
  PROM_SVC=$(kubectl get svc -n monitoring -l app.kubernetes.io/name=prometheus -o jsonpath='{.items[0].metadata.name}')
  kubectl port-forward -n monitoring svc/$PROM_SVC 9090:9090
  Then open: http://localhost:9090

Submit Flink aggregator job manually:
  cd /Users/vijayabhaskarv/IOT/FLUSS/aws-deploy-fluss/high-infra/k8s/flink && ./submit-job-local.sh
